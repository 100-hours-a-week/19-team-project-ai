"""LLM Client adapter for Gemini and other models."""

import asyncio
import json
import logging
import os
from typing import Any

from google import genai
from google.genai import types
from pydantic import BaseModel

logger = logging.getLogger(__name__)

# Fallback ëª¨ë¸ (rate limit ì‹œ ì‹œë„)
FALLBACK_MODELS = ["gemini-2.0-flash-lite"]


class LLMClient:
    """Wrapper for LLM API calls (Gemini) with retry and API key rotation."""

    def __init__(self, model_name: str = "gemini-2.5-flash-lite"):
        self.model_name = model_name
        self._clients: list[genai.Client] = []
        self._client_labels: list[str] = []  # ë””ë²„ê¹…ìš© ë¼ë²¨
        self._current_client_idx = 0
        self._initialized = False
        self.max_retries = 2
        self.base_delay = 1

    def _init_clients(self) -> None:
        """í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (lazy) - ì—¬ëŸ¬ API í‚¤ ì§€ì›"""
        if self._initialized:
            return

        # 1. Vertex AI í´ë¼ì´ì–¸íŠ¸ (ìµœìš°ì„ )
        project_id = os.getenv("GCP_PROJECT_ID")
        location = os.getenv("GCP_LOCATION", "asia-northeast3")
        credentials_path = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")

        if project_id and credentials_path:
            logger.info(f"Using Vertex AI (Project: {project_id}, Location: {location})")
            self._clients.append(genai.Client(vertexai=True, project=project_id, location=location))
            self._client_labels.append(f"VertexAI({location})")

        # 2. ì—¬ëŸ¬ API í‚¤ í´ë¼ì´ì–¸íŠ¸ (GOOGLE_API_KEY, GOOGLE_API_KEY_2, ...)
        api_keys = self._load_api_keys()
        for i, key in enumerate(api_keys):
            self._clients.append(genai.Client(api_key=key))
            label = f"APIKey_{i + 1}({key[:8]}...)"
            self._client_labels.append(label)

        if not self._clients:
            raise ValueError(
                "No LLM client configured. Set GCP_PROJECT_ID/GOOGLE_APPLICATION_CREDENTIALS "
                "or GOOGLE_API_KEY / GOOGLE_API_KEYS."
            )

        logger.info(f"LLM í´ë¼ì´ì–¸íŠ¸ {len(self._clients)}ê°œ ì´ˆê¸°í™”: {self._client_labels}")
        self._initialized = True

    def _load_api_keys(self) -> list[str]:
        """í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ëª©ë¡ ë¡œë“œ"""
        keys = []

        # ë°©ë²• 1: ì½¤ë§ˆ êµ¬ë¶„ (GOOGLE_API_KEYS)
        multi_keys = os.getenv("GOOGLE_API_KEYS", "")
        if multi_keys:
            keys.extend([k.strip() for k in multi_keys.split(",") if k.strip()])

        # ë°©ë²• 2: ê°œë³„ í™˜ê²½ë³€ìˆ˜ (GOOGLE_API_KEY, GOOGLE_API_KEY_2, ...)
        if not keys:
            primary = os.getenv("GOOGLE_API_KEY", "")
            if primary:
                keys.append(primary)
            # _2, _3, ... ìˆœì„œë¡œ íƒìƒ‰
            for i in range(2, 11):
                extra = os.getenv(f"GOOGLE_API_KEY_{i}", "")
                if extra:
                    keys.append(extra)

        return keys

    def _get_client(self) -> genai.Client:
        """í˜„ì¬ active í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜"""
        self._init_clients()
        return self._clients[self._current_client_idx]

    def _rotate_client(self) -> bool:
        """ë‹¤ìŒ í´ë¼ì´ì–¸íŠ¸ë¡œ ì „í™˜. ë‹¤ìŒì´ ìˆìœ¼ë©´ True, ì—†ìœ¼ë©´ False."""
        self._init_clients()
        next_idx = self._current_client_idx + 1
        if next_idx < len(self._clients):
            old_label = self._client_labels[self._current_client_idx]
            self._current_client_idx = next_idx
            new_label = self._client_labels[self._current_client_idx]
            logger.info(f"ğŸ”„ API í‚¤ ì „í™˜: {old_label} â†’ {new_label}")
            return True
        # ëª¨ë‘ ì†Œì§„ â†’ ì²˜ìŒìœ¼ë¡œ ë¦¬ì…‹
        self._current_client_idx = 0
        return False

    async def generate(
        self,
        prompt: str,
        system_instruction: str | None = None,
        temperature: float = 0.1,
        max_tokens: int = 4096,
    ) -> str:
        """Generate text completion."""
        client = self._get_client()

        config = types.GenerateContentConfig(
            temperature=temperature,
            max_output_tokens=max_tokens,
            system_instruction=system_instruction,
        )

        response = await client.aio.models.generate_content(
            model=self.model_name,
            contents=prompt,
            config=config,
        )

        return response.text

    async def generate_json(
        self,
        prompt: str,
        system_instruction: str | None = None,
        response_schema: type[BaseModel] | None = None,
        temperature: float = 0.1,
    ) -> dict[str, Any]:
        """Generate structured JSON output with retry, key rotation, and model fallback."""
        self._init_clients()

        config = types.GenerateContentConfig(
            temperature=temperature,
            response_mime_type="application/json",
            system_instruction=system_instruction,
        )

        if response_schema:
            config.response_schema = response_schema

        # í˜„ì¬ ëª¨ë¸ + fallback ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
        models_to_try = [self.model_name] + [m for m in FALLBACK_MODELS if m != self.model_name]
        last_error = None

        # ëª¨ë“  í´ë¼ì´ì–¸íŠ¸(API í‚¤)ë¥¼ ì‹œë„
        clients_tried = 0
        total_clients = len(self._clients)

        while clients_tried < total_clients:
            client = self._clients[self._current_client_idx]
            client_label = self._client_labels[self._current_client_idx]

            for model in models_to_try:
                for attempt in range(self.max_retries):
                    try:
                        response = await client.aio.models.generate_content(
                            model=model,
                            contents=prompt,
                            config=config,
                        )

                        if model != self.model_name:
                            logger.info(f"âœ… Fallback ëª¨ë¸ ì‚¬ìš© ì„±ê³µ: {model} ({client_label})")

                        text = response.text.strip()
                        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±° ë¡œì§
                        if text.startswith("```"):
                            lines = text.split("\n")
                            if lines[0].startswith("```"):
                                lines = lines[1:]
                            if lines and lines[-1].strip() == "```":
                                lines = lines[:-1]
                            text = "\n".join(lines)

                        return json.loads(text)

                    except Exception as e:
                        last_error = e
                        error_str = str(e).upper()

                        # 429(í• ë‹¹ëŸ‰ ì´ˆê³¼) ë˜ëŠ” 503(ì„œë²„ ê³¼ë¶€í•˜) â†’ ë‹¤ìŒ API í‚¤ë¡œ ì „í™˜
                        if any(code in error_str for code in ["429", "RESOURCE_EXHAUSTED", "503", "OVERLOADED"]):
                            logger.warning(
                                f"âš ï¸ í• ë‹¹ëŸ‰ ì´ˆê³¼ ({client_label}, {model}, ì‹œë„ {attempt + 1}/{self.max_retries})"
                            )

                            # ë§ˆì§€ë§‰ retryë©´ ë‹¤ìŒ API í‚¤ë¡œ ì „í™˜
                            if attempt == self.max_retries - 1:
                                break  # model loop ë°–ìœ¼ë¡œ â†’ client rotation

                            wait_time = self.base_delay * (2**attempt)
                            logger.info(f"â³ {wait_time}ì´ˆ í›„ ì¬ì‹œë„...")
                            await asyncio.sleep(wait_time)
                            continue

                        # ê·¸ ì™¸ ì—ëŸ¬ëŠ” ë‹¤ìŒ ëª¨ë¸ë¡œ
                        logger.warning(f"âš ï¸ {model} í˜¸ì¶œ ì‹¤íŒ¨ ({client_label}): {e}")
                        break

                else:
                    # ëª¨ë“  retry ì„±ê³µ ì—†ì´ ëë‚¨ â†’ ë‹¤ìŒ ëª¨ë¸ ì‹œë„
                    continue
                # 429ë¡œ ì¸í•´ break ëœ ê²½ìš° â†’ ë‹¤ìŒ í´ë¼ì´ì–¸íŠ¸ë¡œ
                break
            else:
                # ëª¨ë“  ëª¨ë¸ ì‹œë„ ì‹¤íŒ¨ â†’ ë‹¤ìŒ í´ë¼ì´ì–¸íŠ¸
                pass

            # ë‹¤ìŒ API í‚¤ë¡œ ì „í™˜
            has_next = self._rotate_client()
            clients_tried += 1

            if has_next and clients_tried < total_clients:
                logger.info(f"ğŸ”„ ë‹¤ìŒ API í‚¤ë¡œ ì „í™˜ ({self._client_labels[self._current_client_idx]})")
            else:
                break

        raise last_error or RuntimeError("ëª¨ë“  API í‚¤ ë° ëª¨ë¸ í˜¸ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

    async def generate_json_with_images(
        self,
        contents: list[types.Part],
        system_instruction: str | None = None,
        response_schema: type[BaseModel] | None = None,
        temperature: float = 0.1,
    ) -> dict[str, Any]:
        """Generate structured JSON output from image+text parts with retry, key rotation, and model fallback."""
        self._init_clients()

        config = types.GenerateContentConfig(
            temperature=temperature,
            response_mime_type="application/json",
            system_instruction=system_instruction,
        )

        if response_schema:
            config.response_schema = response_schema

        models_to_try = [self.model_name] + [m for m in FALLBACK_MODELS if m != self.model_name]
        last_error = None

        clients_tried = 0
        total_clients = len(self._clients)

        while clients_tried < total_clients:
            client = self._clients[self._current_client_idx]
            client_label = self._client_labels[self._current_client_idx]

            for model in models_to_try:
                for attempt in range(self.max_retries):
                    try:
                        response = await client.aio.models.generate_content(
                            model=model,
                            contents=contents,
                            config=config,
                        )

                        if model != self.model_name:
                            logger.info(f"âœ… Fallback ëª¨ë¸ ì‚¬ìš© ì„±ê³µ: {model} ({client_label})")

                        text = response.text.strip()
                        if text.startswith("```"):
                            lines = text.split("\n")
                            if lines[0].startswith("```"):
                                lines = lines[1:]
                            if lines and lines[-1].strip() == "```":
                                lines = lines[:-1]
                            text = "\n".join(lines)

                        return json.loads(text)

                    except Exception as e:
                        last_error = e
                        error_str = str(e).upper()

                        if any(code in error_str for code in ["429", "RESOURCE_EXHAUSTED", "503", "OVERLOADED"]):
                            logger.warning(
                                f"âš ï¸ í• ë‹¹ëŸ‰ ì´ˆê³¼ ({client_label}, {model}, ì‹œë„ {attempt + 1}/{self.max_retries})"
                            )

                            if attempt == self.max_retries - 1:
                                break

                            wait_time = self.base_delay * (2**attempt)
                            logger.info(f"â³ {wait_time}ì´ˆ í›„ ì¬ì‹œë„...")
                            await asyncio.sleep(wait_time)
                            continue

                        logger.warning(f"âš ï¸ {model} í˜¸ì¶œ ì‹¤íŒ¨ ({client_label}): {e}")
                        break

                else:
                    continue
                break
            else:
                pass

            has_next = self._rotate_client()
            clients_tried += 1

            if has_next and clients_tried < total_clients:
                logger.info(f"ğŸ”„ ë‹¤ìŒ API í‚¤ë¡œ ì „í™˜ ({self._client_labels[self._current_client_idx]})")
            else:
                break

        raise last_error or RuntimeError("ëª¨ë“  API í‚¤ ë° ëª¨ë¸ í˜¸ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")


# ì‹±ê¸€í†¤
_llm_client: LLMClient | None = None


def get_llm_client() -> LLMClient:
    """Get or create LLM client singleton."""
    global _llm_client
    if _llm_client is None:
        _llm_client = LLMClient()
    return _llm_client
