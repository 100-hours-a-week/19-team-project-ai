"""LLM Client adapter for Gemini and other models."""

import asyncio
import json
import logging
import os
from typing import Any

from google import genai
from google.genai import types
from pydantic import BaseModel

logger = logging.getLogger(__name__)

# Fallback ëª¨ë¸ ìˆœì„œ (rate limit ì‹œ ìˆœì°¨ ì‹œë„)
FALLBACK_MODELS = [
    "gemini-2.0-flash",
    "gemini-2.0-flash-lite",  # ë” ê°€ë²¼ìš´ ëª¨ë¸
]


class LLMClient:
    """Wrapper for LLM API calls (Gemini) with retry and fallback."""

    def __init__(self, model_name: str = "gemini-2.0-flash"):
        self.model_name = model_name
        self._client: genai.Client | None = None
        self.max_retries = 5  # ìž¬ì‹œë„ íšŸìˆ˜ ìƒí–¥
        self.base_delay = 3  # ê¸°ë³¸ ëŒ€ê¸° ì‹œê°„ ìƒí–¥ (ì´ˆ)

    def _get_client(self) -> genai.Client:
        """Get or create the Gemini client (lazy initialization)."""
        if self._client is None:
            # Vertex AI ì„¤ì • í™•ì¸
            project_id = os.getenv("GCP_PROJECT_ID")
            location = os.getenv("GCP_LOCATION", "asia-northeast3")
            credentials_path = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")

            if project_id and credentials_path:
                logger.info(f"Using Vertex AI (Project: {project_id}, Location: {location})")
                self._client = genai.Client(
                    vertexai=True,
                    project=project_id,
                    location=location,
                )
            else:
                # ê¸°ì¡´ API í‚¤ ë°©ì‹
                api_key = os.getenv("GOOGLE_API_KEY")
                if not api_key:
                    raise ValueError(
                        "Neither Vertex AI (GCP_PROJECT_ID/GOOGLE_APPLICATION_CREDENTIALS) "
                        "nor Gemini API Key (GOOGLE_API_KEY) is set."
                    )
                logger.info("Using Gemini API Key")
                self._client = genai.Client(api_key=api_key)
        return self._client

    async def generate(
        self,
        prompt: str,
        system_instruction: str | None = None,
        temperature: float = 0.1,
        max_tokens: int = 4096,
    ) -> str:
        """Generate text completion."""
        client = self._get_client()

        config = types.GenerateContentConfig(
            temperature=temperature,
            max_output_tokens=max_tokens,
            system_instruction=system_instruction,
        )

        response = await client.aio.models.generate_content(
            model=self.model_name,
            contents=prompt,
            config=config,
        )

        return response.text

    async def generate_json(
        self,
        prompt: str,
        system_instruction: str | None = None,
        response_schema: type[BaseModel] | None = None,
        temperature: float = 0.1,
    ) -> dict[str, Any]:
        """Generate structured JSON output with robust retry for Vertex AI quotas."""
        client = self._get_client()

        config = types.GenerateContentConfig(
            temperature=temperature,
            response_mime_type="application/json",
            system_instruction=system_instruction,
        )

        if response_schema:
            config.response_schema = response_schema

        # í˜„ìž¬ ëª¨ë¸ë¶€í„° ì‹œìž‘í•˜ëŠ” fallback ë¦¬ìŠ¤íŠ¸ ìƒì„± (2.0 ëª¨ë¸êµ° ìœ„ì£¼)
        models_to_try = [self.model_name]
        for model in FALLBACK_MODELS:
            if model not in models_to_try:
                models_to_try.append(model)

        last_error = None

        for model in models_to_try:
            for attempt in range(self.max_retries):
                try:
                    # Vertex AIëŠ” 'gemini-2.0-flash' ê°™ì€ ì§§ì€ ì´ë¦„ ëŒ€ì‹ 
                    # 'publishers/google/models/...' í˜•ì‹ì„ ê¸°ëŒ€í•  ìˆ˜ ìžˆìœ¼ë‚˜ SDKê°€ ë³€í™˜í•¨
                    response = await client.aio.models.generate_content(
                        model=model,
                        contents=prompt,
                        config=config,
                    )

                    if model != self.model_name:
                        logger.info(f"âœ… Fallback ëª¨ë¸ ì‚¬ìš© ì„±ê³µ: {model}")

                    text = response.text.strip()
                    # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±° ë¡œì§
                    if text.startswith("```"):
                        lines = text.split("\n")
                        if lines[0].startswith("```"):
                            lines = lines[1:]
                        if lines and lines[-1].strip() == "```":
                            lines = lines[:-1]
                        text = "\n".join(lines)

                    return json.loads(text)

                except Exception as e:
                    last_error = e
                    error_str = str(e).upper()

                    # 429(í• ë‹¹ëŸ‰ ì´ˆê³¼) ë˜ëŠ” 503(ì„œë²„ ê³¼ë¶€í•˜) ì—ëŸ¬ ì²˜ë¦¬
                    if any(code in error_str for code in ["429", "RESOURCE_EXHAUSTED", "503", "OVERLOADED"]):
                        # ì§€ìˆ˜ ë°±ì˜¤í”„: 2^attempt * base_delay (ì˜ˆ: 3, 6, 12, 24, 48ì´ˆ)
                        # Vertex AIì˜ ê²½ìš° 1.5-flashëŠ” ë„‰ë„‰í•˜ì§€ë§Œ 2.0ì€ Tierì— ë”°ë¼ ì¢ì„ ìˆ˜ ìžˆìŒ
                        wait_time = self.base_delay * (2**attempt)
                        logger.warning(
                            f"âš ï¸ í• ë‹¹ëŸ‰ ì´ˆê³¼ ë˜ëŠ” ì„œë²„ ê³¼ë¶€í•˜ ({model}, ì‹œë„ {attempt + 1}/{self.max_retries}). "
                            f"{wait_time}ì´ˆ í›„ ë‹¤ì‹œ ì‹œë„í•©ë‹ˆë‹¤... ì—ëŸ¬ ë©”ì‹œì§€: {e}"
                        )
                        await asyncio.sleep(wait_time)
                        continue

                    # ê·¸ ì™¸ì˜ ì—ëŸ¬ (ì˜ˆ: 404 ëª¨ë¸ ì—†ìŒ)ëŠ” ì¦‰ì‹œ ë‹¤ìŒ ëª¨ë¸ë¡œ ì „í™˜
                    logger.error(f"âŒ ëª¨ë¸ {model} í˜¸ì¶œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬ ë°œìƒ: {e}")
                    break

            if model != models_to_try[-1]:
                logger.info(f"ðŸ”„ ëª¨ë¸ {model}ì˜ ëª¨ë“  ì‹œë„ ì‹¤íŒ¨. ë‹¤ìŒ Fallback ëª¨ë¸({models_to_try[models_to_try.index(model)+1]})ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")

        raise last_error or RuntimeError("ëª¨ë“  Vertex AI ëª¨ë¸ í˜¸ì¶œ ë° ìž¬ì‹œë„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")


# Singleton instance
_llm_client: LLMClient | None = None


def get_llm_client() -> LLMClient:
    """Get or create LLM client singleton."""
    global _llm_client
    if _llm_client is None:
        _llm_client = LLMClient()
    return _llm_client
