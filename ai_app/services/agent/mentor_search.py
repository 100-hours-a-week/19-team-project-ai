"""D1 ë©˜í†  íƒìƒ‰ íŒŒì´í”„ë¼ì¸ â€” ì¡°ê±´ ê¸°ë°˜ ë²¡í„° ê²€ìƒ‰ + ë£° ì¬ì •ë ¬ + SSE ìŠ¤íŠ¸ë¦¬ë°"""

import json
import logging
from collections.abc import AsyncGenerator
from datetime import datetime, timezone
from typing import Any

from adapters.llm_client import LLMClient, get_llm_client
from prompts import load_prompt
from schemas.agent import MentorCard, MentorConditions
from sqlalchemy import text
from sqlalchemy.engine import Connection

from services.agent.slot_filling import SlotFiller
from services.reco.embedder import ProfileEmbedder, get_embedder

logger = logging.getLogger(__name__)

# ============== ì¿¼ë¦¬ ë¹Œë“œ ==============


def build_query_text(conditions: MentorConditions) -> str:
    """
    ì¶”ì¶œëœ ì¡°ê±´ì„ ì„ë² ë”©ìš© ê²€ìƒ‰ ì¿¼ë¦¬ í…ìŠ¤íŠ¸ë¡œ ì¡°í•©í•œë‹¤.

    ì˜ˆ: "ì§ë¬´: ë°±ì—”ë“œ. ê¸°ìˆ ìŠ¤íƒ: Spring, MSA. ê²½ë ¥: 3ë…„"
    """
    parts = []

    if conditions.job:
        parts.append(f"ì§ë¬´: {conditions.job}")
    if conditions.skills:
        parts.append(f"ê¸°ìˆ ìŠ¤íƒ: {', '.join(conditions.skills)}")
    if conditions.experience_years is not None:
        parts.append(f"ê²½ë ¥: {conditions.experience_years}ë…„")
    if conditions.domain:
        parts.append(f"ë„ë©”ì¸: {conditions.domain}")
    if conditions.region:
        parts.append(f"ì§€ì—­: {conditions.region}")
    if conditions.company_type:
        parts.append(f"íšŒì‚¬ìœ í˜•: {conditions.company_type}")
    if conditions.keywords:
        parts.append(f"í‚¤ì›Œë“œ: {', '.join(conditions.keywords)}")

    query = ". ".join(parts) if parts else "ë©˜í†  ì¶”ì²œ"
    logger.info(f"ê²€ìƒ‰ ì¿¼ë¦¬: {query}")
    return query


# ============== ë²¡í„° ê²€ìƒ‰ ==============

_VECTOR_SEARCH_QUERY = """
    SELECT
        u.id as user_id,
        u.nickname,
        u.introduction,
        ep.company_name,
        ep.verified,
        ep.rating_avg,
        ep.rating_count,
        ep.responded_request_count,
        ep.accepted_request_count,
        ep.rejected_request_count,
        ep.last_active_at,
        1 - (ep.embedding <=> CAST(:query_embedding AS vector)) as embedding_similarity,
        ARRAY_AGG(DISTINCT s.name) FILTER (WHERE s.name IS NOT NULL) as skills,
        ARRAY_AGG(DISTINCT j.name) FILTER (WHERE j.name IS NOT NULL) as jobs
    FROM expert_profiles ep
    JOIN users u ON ep.user_id = u.id
    LEFT JOIN user_skills us ON u.id = us.user_id
    LEFT JOIN skills s ON us.skill_id = s.id
    LEFT JOIN user_jobs uj ON u.id = uj.user_id
    LEFT JOIN jobs j ON uj.job_id = j.id
    WHERE ep.embedding IS NOT NULL
    GROUP BY u.id, u.nickname, u.introduction,
             ep.company_name, ep.verified, ep.rating_avg, ep.rating_count,
             ep.responded_request_count, ep.accepted_request_count,
             ep.rejected_request_count, ep.last_active_at, ep.embedding
    ORDER BY ep.embedding <=> CAST(:query_embedding AS vector)
    LIMIT :top_n
"""


def vector_search(
    query_embedding: list[float],
    conn: Connection,
    top_n: int = 50,
) -> list[dict[str, Any]]:
    """
    pgvectorì—ì„œ ë©˜í†  í›„ë³´ Top N ê²€ìƒ‰

    Args:
        query_embedding: ê²€ìƒ‰ ì¿¼ë¦¬ ì„ë² ë”© ë²¡í„°
        conn: SQLAlchemy DB ì—°ê²°
        top_n: ê²€ìƒ‰ í›„ë³´ ìˆ˜

    Returns:
        ë©˜í†  í›„ë³´ ë¦¬ìŠ¤íŠ¸
    """
    result = conn.execute(
        text(_VECTOR_SEARCH_QUERY),
        {
            "query_embedding": str(query_embedding),
            "top_n": top_n,
        },
    )

    candidates = []
    for row in result:
        response_rate = 0.0
        if row.responded_request_count and row.responded_request_count > 0:
            response_rate = row.accepted_request_count / row.responded_request_count * 100

        candidates.append(
            {
                "user_id": row.user_id,
                "nickname": row.nickname,
                "introduction": row.introduction or "",
                "company_name": row.company_name,
                "verified": row.verified,
                "rating_avg": round(row.rating_avg, 1) if row.rating_avg else 0.0,
                "rating_count": row.rating_count or 0,
                "response_rate": round(response_rate, 1),
                "skills": row.skills or [],
                "jobs": row.jobs or [],
                "similarity_score": round(float(row.embedding_similarity), 4),
                "last_active_at": row.last_active_at,
            }
        )

    logger.info(f"ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ: {len(candidates)}ëª… í›„ë³´")
    return candidates


# ============== ë£° ê¸°ë°˜ ì¬ì •ë ¬ ==============


def rule_rerank(
    candidates: list[dict],
    conditions: MentorConditions,
    top_k: int = 3,
) -> list[MentorCard]:
    """
    ë£° ê¸°ë°˜ ì¬ì •ë ¬

    ì¬ì •ë ¬ ê·œì¹™:
    1. ì§ë¬´ ì¼ì¹˜ ê°€ì¤‘ì¹˜ (+0.15)
    2. ê¸°ìˆ ìŠ¤íƒ ì¼ì¹˜ ê°€ì¤‘ì¹˜ (+0.05 Ã— ì¼ì¹˜ ê°œìˆ˜)
    3. ê²½ë ¥ ì°¨ì´ íŒ¨ë„í‹° (ì°¨ì´ Ã— -0.02, ìµœëŒ€ -0.1)
    4. ì°¨ì´ê°€ í° ê²½ìš° ì‘ë‹µë¥  ìš°ì„  ë³´ì •
    5. ìµœê·¼ í™œë™ ê°€ì¤‘ì¹˜ (7ì¼ ì´ë‚´ +0.05)

    Args:
        candidates: ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ í›„ë³´
        conditions: ì‚¬ìš©ì ìš”ì²­ ì¡°ê±´
        top_k: ìµœì¢… ë°˜í™˜ ìˆ˜

    Returns:
        ì¬ì •ë ¬ëœ MentorCard ë¦¬ìŠ¤íŠ¸
    """
    scored = []
    user_skills = set(s.lower() for s in conditions.skills) if conditions.skills else set()
    user_job = conditions.job.lower() if conditions.job else None

    for cand in candidates:
        score = cand["similarity_score"]
        filter_type = None

        # 1. ì§ë¬´ ì¼ì¹˜ ê°€ì¤‘ì¹˜
        mentor_jobs = set(j.lower() for j in cand.get("jobs", []))
        if user_job and user_job in mentor_jobs:
            score += 0.15
            filter_type = "job"

        # 2. ê¸°ìˆ ìŠ¤íƒ ì¼ì¹˜ ê°€ì¤‘ì¹˜
        mentor_skills = set(s.lower() for s in cand.get("skills", []))
        skill_overlap = len(user_skills & mentor_skills)
        if skill_overlap > 0:
            score += 0.05 * skill_overlap
            if filter_type is None:
                filter_type = "skill"

        # 3. ê²½ë ¥ ì°¨ì´ íŒ¨ë„í‹° (ë©˜í† ì˜ ê²½ë ¥ ì •ë³´ ì—†ìœ¼ë©´ íŒ¨ìŠ¤)
        # ì°¸ê³ : í˜„ì¬ DBì— ê²½ë ¥ ì—°ìˆ˜ í•„ë“œê°€ ì—†ìœ¼ë¯€ë¡œ ì´ ë¡œì§ì€ ì¶”í›„ í™•ì¥ ê°€ëŠ¥

        # 4. ì‘ë‹µë¥  ë³´ì • (ì§ë¬´/ìŠ¤íƒ ëª¨ë‘ ë¶ˆì¼ì¹˜ì¸ ê²½ìš°)
        if filter_type is None and cand["response_rate"] > 0:
            score += cand["response_rate"] / 1000  # ìµœëŒ€ +0.1
            filter_type = "response_rate"

        # 5. ìµœê·¼ í™œë™ ê°€ì¤‘ì¹˜
        if cand.get("last_active_at"):
            try:
                last_active = cand["last_active_at"]
                if hasattr(last_active, "tzinfo") and last_active.tzinfo is None:
                    last_active = last_active.replace(tzinfo=timezone.utc)
                days_ago = (datetime.now(timezone.utc) - last_active).days
                if days_ago <= 7:
                    score += 0.05
            except Exception:
                pass

        scored.append(
            {
                **cand,
                "rerank_score": round(score, 4),
                "filter_type": filter_type,
            }
        )

    # ì¬ì •ë ¬ ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    scored.sort(key=lambda x: x["rerank_score"], reverse=True)

    # Top K ì„ íƒ & MentorCard ë³€í™˜
    cards = []
    for item in scored[:top_k]:
        cards.append(
            MentorCard(
                user_id=item["user_id"],
                nickname=item["nickname"],
                company_name=item.get("company_name"),
                verified=item.get("verified", False),
                rating_avg=item.get("rating_avg", 0.0),
                rating_count=item.get("rating_count", 0),
                response_rate=item.get("response_rate", 0.0),
                skills=item.get("skills", []),
                jobs=item.get("jobs", []),
                introduction=item.get("introduction", ""),
                similarity_score=item["similarity_score"],
                rerank_score=item["rerank_score"],
                filter_type=item.get("filter_type"),
            )
        )

    logger.info(f"ì¬ì •ë ¬ ì™„ë£Œ: {len(cards)}ëª… ìµœì¢… ì„ íƒ")
    return cards


# ============== ì¶”ê°€ ì¡°ê±´ ì•ˆë‚´ ==============


def check_need_more_conditions(conditions: MentorConditions) -> str | None:
    """
    ì¡°ê±´ì´ ë„“ìœ¼ë©´ ì¶”ê°€ ì•ˆë‚´ ë¬¸êµ¬ë¥¼ ìƒì„±í•œë‹¤.

    Returns:
        ì¶”ê°€ ì•ˆë‚´ ë¬¸êµ¬ ë˜ëŠ” None
    """
    missing = []

    if not conditions.job:
        missing.append("ì§ë¬´(ë°±ì—”ë“œ, í”„ë¡ íŠ¸ì—”ë“œ ë“±)")
    if not conditions.skills:
        missing.append("ê¸°ìˆ ìŠ¤íƒ(Spring, React ë“±)")
    if conditions.experience_years is None:
        missing.append("ê²½ë ¥(3ë…„ì°¨ ì´ìƒ ë“±)")
    if not conditions.domain:
        missing.append("ë„ë©”ì¸(í•€í…Œí¬, ì´ì»¤ë¨¸ìŠ¤ ë“±)")

    if len(missing) >= 3:
        return f"ğŸ’¡ ë” ì •í™•í•œ ì¶”ì²œì„ ìœ„í•´ ë‹¤ìŒ ì¡°ê±´ì„ ì¶”ê°€ë¡œ ì•Œë ¤ì£¼ì‹œë©´ ì¢‹ì•„ìš”: {', '.join(missing[:3])}"
    elif len(missing) >= 2:
        return f"ğŸ’¡ {', '.join(missing)}ì„(ë¥¼) ì¶”ê°€ë¡œ ì•Œë ¤ì£¼ì‹œë©´ ë” ì •í™•í•œ ì¶”ì²œì´ ê°€ëŠ¥í•´ìš”!"

    return None


# ============== ë©˜íŠ¸ ìƒì„± ==============


async def compose_reply_text(
    conditions: MentorConditions,
    cards: list[MentorCard],
    need_more: str | None,
    llm: LLMClient | None = None,
) -> str:
    """
    ë©˜í†  ì¹´ë“œ + ì¡°ê±´ì„ ë°”íƒ•ìœ¼ë¡œ ìì—°ì–´ ë©˜íŠ¸ë¥¼ ìƒì„±í•œë‹¤.

    Args:
        conditions: ì¶”ì¶œëœ ì¡°ê±´
        cards: ì¶”ì²œ ë©˜í†  ì¹´ë“œ
        need_more: ì¶”ê°€ ì¡°ê±´ ì•ˆë‚´ ë¬¸êµ¬ (ì—†ìœ¼ë©´ None)
        llm: LLM í´ë¼ì´ì–¸íŠ¸

    Returns:
        ìì—°ì–´ ë©˜íŠ¸ ë¬¸ìì—´
    """
    llm = llm or get_llm_client()
    system_prompt = load_prompt("mentor_card_system")

    # ì¹´ë“œ ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    cards_text = json.dumps(
        [c.model_dump() for c in cards],
        ensure_ascii=False,
        indent=2,
    )

    conditions_text = json.dumps(
        conditions.model_dump(exclude_none=True),
        ensure_ascii=False,
    )

    user_prompt = f"""## ì‚¬ìš©ì ì¡°ê±´
{conditions_text}

## ì¶”ì²œ ë©˜í†  ì¹´ë“œ ({len(cards)}ëª…)
{cards_text}

## ì¶”ê°€ ì¡°ê±´ í•„ìš” ì—¬ë¶€
{"ìˆìŒ: " + need_more if need_more else "ì—†ìŒ (ì¡°ê±´ ì¶©ë¶„)"}

ìœ„ ë°ì´í„°ë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ì¶”ì²œ ë©˜íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”."""

    try:
        reply = await llm.generate(
            prompt=user_prompt,
            system_instruction=system_prompt,
            temperature=0.7,
            max_tokens=1024,
        )
        return reply.strip()
    except Exception as e:
        logger.error(f"ë©˜íŠ¸ ìƒì„± ì‹¤íŒ¨, ê¸°ë³¸ ë©˜íŠ¸ ë°˜í™˜: {e}")
        # fallback ë©˜íŠ¸
        return _fallback_reply(conditions, cards, need_more)


def _fallback_reply(
    conditions: MentorConditions,
    cards: list[MentorCard],
    need_more: str | None,
) -> str:
    """LLM ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë©˜íŠ¸"""
    parts = []

    # ì¡°ê±´ ìš”ì•½
    cond_parts = []
    if conditions.job:
        cond_parts.append(conditions.job)
    if conditions.skills:
        cond_parts.append(", ".join(conditions.skills))
    if conditions.experience_years:
        cond_parts.append(f"{conditions.experience_years}ë…„ì°¨")

    if cond_parts:
        parts.append(f"{'Â·'.join(cond_parts)} ì¡°ê±´ìœ¼ë¡œ ë©˜í† ë¥¼ ì°¾ì•˜ì–´ìš”! ğŸ¯\n")
    else:
        parts.append("ë©˜í† ë¥¼ ì°¾ì•˜ì–´ìš”! ğŸ¯\n")

    # ì¹´ë“œ ì†Œê°œ
    for i, card in enumerate(cards, 1):
        emoji = ["1ï¸âƒ£", "2ï¸âƒ£", "3ï¸âƒ£", "4ï¸âƒ£", "5ï¸âƒ£"][i - 1] if i <= 5 else f"{i}."
        verified = " (âœ… ì¸ì¦)" if card.verified else ""
        company = f" â€” {card.company_name}" if card.company_name else ""
        skills_str = ", ".join(card.skills[:3]) if card.skills else ""
        parts.append(
            f"{emoji} **{card.nickname}**{company}{verified} | {skills_str} | ë§¤ì¹­ {int(card.rerank_score * 100)}%"
        )

    if need_more:
        parts.append(f"\n{need_more}")

    return "\n".join(parts)


# ============== D1 ì „ì²´ íŒŒì´í”„ë¼ì¸ (SSE ìŠ¤íŠ¸ë¦¬ë°) ==============


async def run_d1_pipeline(
    message: str,
    conn: Connection,
    top_k: int = 3,
    top_n: int = 50,
    llm: LLMClient | None = None,
    embedder: ProfileEmbedder | None = None,
) -> AsyncGenerator[dict, None]:
    """
    D1 ì¡°ê±´ ê¸°ë°˜ ë©˜í†  íƒìƒ‰ íŒŒì´í”„ë¼ì¸ (SSE ì´ë²¤íŠ¸ ìƒì„±ê¸°)

    íë¦„: ì¡°ê±´ ì¶”ì¶œ â†’ ì¿¼ë¦¬ ë¹Œë“œ â†’ ì„ë² ë”© â†’ ë²¡í„° ê²€ìƒ‰ â†’ ë£° ì¬ì •ë ¬ â†’ ì¹´ë“œ ë Œë” â†’ ë©˜íŠ¸ ìƒì„±

    Yields:
        SSE ì´ë²¤íŠ¸ dict: {"event": str, "data": dict}
    """
    llm = llm or get_llm_client()
    embedder = embedder or get_embedder()

    # 1. ì¡°ê±´ ì¶”ì¶œ
    slot_filler = SlotFiller(llm=llm)
    conditions = await slot_filler.extract(message)

    yield {
        "event": "conditions",
        "data": conditions.model_dump(exclude_none=True),
    }

    # 2. ì¿¼ë¦¬ ë¹Œë“œ & ì„ë² ë”© ìƒì„±
    query_text = build_query_text(conditions)
    query_embedding = embedder.embed_text(query_text)
    embedding_list = query_embedding.tolist()

    # 3. ë²¡í„° ê²€ìƒ‰ Top N
    candidates = vector_search(embedding_list, conn, top_n=top_n)

    if not candidates:
        yield {
            "event": "text",
            "data": {"chunk": "ì¡°ê±´ì— ë§ëŠ” ë©˜í† ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”. ì¡°ê±´ì„ ë³€ê²½í•´ì„œ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”! ğŸ™"},
        }
        yield {"event": "done", "data": {}}
        return

    # 4. ë£° ê¸°ë°˜ ì¬ì •ë ¬ â†’ Top K
    cards = rule_rerank(candidates, conditions, top_k=top_k)

    # 5. ì¹´ë“œ ì „ì†¡
    yield {
        "event": "cards",
        "data": {"cards": [c.model_dump() for c in cards]},
    }

    # 6. ì¶”ê°€ ì¡°ê±´ í•„ìš” ì—¬ë¶€
    need_more = check_need_more_conditions(conditions)

    # 7. ìì—°ì–´ ë©˜íŠ¸ ìƒì„± & ìŠ¤íŠ¸ë¦¬ë°
    reply_text = await compose_reply_text(conditions, cards, need_more, llm=llm)

    # ì²­í¬ ë‹¨ìœ„ë¡œ ìŠ¤íŠ¸ë¦¬ë° (ë¬¸ì¥ ë‹¨ìœ„)
    sentences = reply_text.split("\n")
    for sentence in sentences:
        if sentence.strip():
            yield {
                "event": "text",
                "data": {"chunk": sentence + "\n"},
            }

    # 8. ì™„ë£Œ
    yield {"event": "done", "data": {}}
