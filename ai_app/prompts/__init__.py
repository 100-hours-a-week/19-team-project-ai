"""í”„ë¡¬í”„íŠ¸ ë¡œë” - ì™¸ë¶€ .md íŒŒì¼ì—ì„œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ"""

import logging
import os
from functools import lru_cache
from pathlib import Path

from langfuse import Langfuse

logger = logging.getLogger(__name__)

PROMPTS_DIR = Path(__file__).parent

# Langfuse í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (í™˜ê²½ ë³€ìˆ˜ ê¸°ë°˜)
langfuse = Langfuse(
    public_key=os.getenv("LANGFUSE_PUBLIC_KEY"),
    secret_key=os.getenv("LANGFUSE_SECRET_KEY"),
    host=os.getenv("LANGFUSE_HOST", "https://us.cloud.langfuse.com"),
)


@lru_cache(maxsize=32)
def load_prompt(name: str) -> str:
    """
    í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¡œë“œ (ìºì‹±ë¨)
    Langfuseì—ì„œ ë¨¼ì € ì‹œë„í•˜ê³ , ì‹¤íŒ¨ ì‹œ ë¡œì»¬ .md íŒŒì¼ì—ì„œ ë¡œë“œ(Fallback).

    Args:
        name: í”„ë¡¬í”„íŠ¸ íŒŒì¼ëª… (í™•ì¥ì ì œì™¸)
              ì˜ˆ: "resume_extraction_system"

    Returns:
        í”„ë¡¬í”„íŠ¸ ë‚´ìš©
    """
    # 1. Langfuseì—ì„œ ë¡œë“œ ì‹œë„
    try:
        # fetch_load=Trueë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì‹  ë²„ì „ì„ ëª…ì‹œì ìœ¼ë¡œ ê°€ì ¸ì˜´
        prompt = langfuse.get_prompt(name, type="chat" if "_system" in name or "_user" in name else "text")
        if prompt:
            logger.info(f"âœ… Loaded prompt '{name}' from Langfuse (v{prompt.version})")
            # Langchain prompt template í¬ë§·ì¸ ê²½ìš° .promptë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜, ì¼ë°˜ í…ìŠ¤íŠ¸ì¸ ê²½ìš° ì²˜ë¦¬
            # ì—¬ê¸°ì„œëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ë¬¸ìì—´ ë°˜í™˜ì„ ìœ„í•´ compile() í›„ content ì¶”ì¶œ ë˜ëŠ” raw content ì‚¬ìš©
            if hasattr(prompt, "get_langchain_prompt"):
                return prompt.compile()
            return prompt.prompt
    except Exception as e:
        logger.warning(f"âš ï¸ Failed to load prompt '{name}' from Langfuse, falling back to local: {e}")

    # 2. ë¡œì»¬ Fallback
    file_path = PROMPTS_DIR / f"{name}.md"
    if not file_path.exists():
        logger.error(f"âŒ Prompt file not found: {file_path}")
        raise FileNotFoundError(f"Prompt file not found: {file_path}")

    logger.info(f"ğŸ“ Loaded prompt '{name}' from local file")
    return file_path.read_text(encoding="utf-8")


def get_resume_extraction_prompts() -> tuple[str, str]:
    """
    ì´ë ¥ì„œ ì¶”ì¶œìš© í”„ë¡¬í”„íŠ¸ ë¡œë“œ

    Returns:
        (system_prompt, user_prompt_template) íŠœí”Œ
    """
    system_prompt = load_prompt("resume_extraction_system")
    user_prompt = load_prompt("resume_extraction_user")
    return system_prompt, user_prompt


def get_vlm_ocr_pii_prompts() -> tuple[str, str]:
    """
    VLM OCR + PII íƒì§€ìš© í”„ë¡¬í”„íŠ¸ ë¡œë“œ

    Returns:
        (system_prompt, user_prompt) íŠœí”Œ
    """
    system_prompt = load_prompt("vlm_ocr_pii_system")
    user_prompt = load_prompt("vlm_ocr_pii_user")
    return system_prompt, user_prompt


def clear_prompt_cache():
    """í”„ë¡¬í”„íŠ¸ ìºì‹œ ì´ˆê¸°í™” (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)"""
    load_prompt.cache_clear()
